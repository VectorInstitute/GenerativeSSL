#!/bin/bash

#SBATCH --job-name="multi_eval"
#SBATCH --partition=a40
#SBATCH --qos=m2
#SBATCH --nodes=2
#SBATCH --gres=gpu:a40:4
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=32
#SBATCH --mem=0
#SBATCH --output=multinode-%j.out
#SBATCH --error=multinode-%j.err
#SBATCH --open-mode=append
#SBATCH --wait-all-nodes=1
#SBATCH --time=08:00:00

# load virtual environment
source /ssd003/projects/aieng/envs/genssl2/bin/activate

export NCCL_IB_DISABLE=1  # Our cluster does not have InfiniBand. We need to disable usage using this flag.
export TORCH_NCCL_ASYNC_ERROR_HANDLING=1 # set to 1 for NCCL backend
# export CUDA_LAUNCH_BLOCKING=1


export MASTER_ADDR="$(hostname --fqdn)"
export MASTER_PORT="$(python -c 'import socket; s=socket.socket(); s.bind(("", 0)); print(s.getsockname()[1])')"
export RDVZ_ID=$RANDOM
echo "RDZV Endpoint $MASTER_ADDR:$MASTER_PORT"

export PYTHONPATH="."
nvidia-smi

srun -p $SLURM_JOB_PARTITION \
    -c $SLURM_CPUS_ON_NODE \
    -N $SLURM_JOB_NUM_NODES \
    --mem=0 \
    --gres=gpu:$SLURM_JOB_PARTITION:$SLURM_GPUS_ON_NODE \
    bash -c 'torchrun \
    --nproc-per-node=$SLURM_GPUS_ON_NODE \
    --nnodes=$SLURM_JOB_NUM_NODES \
    --rdzv-endpoint $MASTER_ADDR:$MASTER_PORT \
    --rdzv-id $RDVZ_ID \
    --rdzv-backend c10d \
   simsiam/adil_eval.py \
   --data_dir=/scratch/ssd004/datasets/imagenet256 \
   --arch=resnet50 \
   --distributed_mode \
   --batch-size=256 \
   --epochs=100 \
   --pretrained_checkpoint=/ssd003/projects/aieng/arash/checkpoint_0099.pth.tar'