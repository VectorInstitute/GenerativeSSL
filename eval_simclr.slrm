#!/bin/bash

#SBATCH --job-name=train_sunrgbd
#SBATCH --partition=t4v2
#SBATCH --time=12:00:00
#SBATCH --nodes=1
#SBATCH --gres=gpu:4
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=100G
#SBATCH --output=./logs/simclr/eval_slurm-%N-%j.out
#SBATCH --error=./logs/simclr/eval_slurm-%N-%j.err
#SBATCH --qos=m

PY_ARGS=${@:1}

# load virtual environment
source /ssd003/projects/aieng/envs/genssl2/bin/activate

export TORCH_NCCL_ASYNC_ERROR_HANDLING=1 # set to 1 for NCCL backend
export CUDA_LAUNCH_BLOCKING=1

export MASTER_ADDR=$(hostname)
export MASTER_PORT=45679

export PYTHONPATH="."
nvidia-smi

files=$(ls checkpoint_epoch_*)

# Loop through each file and pass it as a parameter to the rest of the script
for file in $files
do
    # torchrun execute nproc-per-node * nodes times
    torchrun --nnodes 1 --nproc-per-node 4 evaluate_simCLR.py \
    --distributed_mode \
    --distributed_launcher="pytorch" \
    --batch-size=256 \
    --pretrained_model_dir="/projects/imagenet_synthetic/train_models" \
    --experiment_name="simclr/2024_02_23_13_02" \
    --pretrained_model_name=$file \
    --linear_evaluation \
    --arch="resnet50"
    # Add your processing logic here
done